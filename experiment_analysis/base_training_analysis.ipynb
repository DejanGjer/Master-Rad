{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5e8c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sweep_dir = \"..\\\\base_training\\\\sweep_2025-07-08_00-24-53\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee17f6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cifar100\n",
      "Results DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dejan\\AppData\\Local\\Temp\\ipykernel_1452\\1572324523.py:73: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>weight decay</th>\n",
       "      <th>batch size</th>\n",
       "      <th>normal</th>\n",
       "      <th>negative</th>\n",
       "      <th>hybrid normal</th>\n",
       "      <th>hybrid negative</th>\n",
       "      <th>synergy normal</th>\n",
       "      <th>synergy negative</th>\n",
       "      <th>synergy all</th>\n",
       "      <th>synergy trained all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-08_00-24-53</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>73.52</td>\n",
       "      <td>73.39</td>\n",
       "      <td>73.29</td>\n",
       "      <td>73.01</td>\n",
       "      <td>73.36</td>\n",
       "      <td>73.16</td>\n",
       "      <td>75.86</td>\n",
       "      <td>75.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-08_00-24-55</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.001</td>\n",
       "      <td>768</td>\n",
       "      <td>69.67</td>\n",
       "      <td>70.15</td>\n",
       "      <td>69.68</td>\n",
       "      <td>69.70</td>\n",
       "      <td>69.56</td>\n",
       "      <td>70.03</td>\n",
       "      <td>71.83</td>\n",
       "      <td>71.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-08_00-24-56</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>72.36</td>\n",
       "      <td>71.67</td>\n",
       "      <td>72.41</td>\n",
       "      <td>71.75</td>\n",
       "      <td>72.48</td>\n",
       "      <td>71.73</td>\n",
       "      <td>74.53</td>\n",
       "      <td>74.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-08_00-24-57</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.005</td>\n",
       "      <td>768</td>\n",
       "      <td>72.28</td>\n",
       "      <td>72.11</td>\n",
       "      <td>72.05</td>\n",
       "      <td>72.25</td>\n",
       "      <td>72.07</td>\n",
       "      <td>72.16</td>\n",
       "      <td>74.25</td>\n",
       "      <td>74.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-08_00-24-58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>67.64</td>\n",
       "      <td>68.41</td>\n",
       "      <td>68.18</td>\n",
       "      <td>68.50</td>\n",
       "      <td>67.72</td>\n",
       "      <td>68.47</td>\n",
       "      <td>70.05</td>\n",
       "      <td>70.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-07-08_00-24-59</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.010</td>\n",
       "      <td>768</td>\n",
       "      <td>72.82</td>\n",
       "      <td>72.18</td>\n",
       "      <td>72.78</td>\n",
       "      <td>71.93</td>\n",
       "      <td>72.80</td>\n",
       "      <td>72.28</td>\n",
       "      <td>74.56</td>\n",
       "      <td>74.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-07-08_00-25-00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>72.95</td>\n",
       "      <td>72.69</td>\n",
       "      <td>72.71</td>\n",
       "      <td>72.32</td>\n",
       "      <td>73.00</td>\n",
       "      <td>72.54</td>\n",
       "      <td>75.21</td>\n",
       "      <td>74.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-07-08_00-25-01</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>768</td>\n",
       "      <td>71.82</td>\n",
       "      <td>71.68</td>\n",
       "      <td>71.75</td>\n",
       "      <td>71.39</td>\n",
       "      <td>71.86</td>\n",
       "      <td>71.43</td>\n",
       "      <td>74.12</td>\n",
       "      <td>73.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-07-08_00-25-02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>61.38</td>\n",
       "      <td>61.22</td>\n",
       "      <td>61.83</td>\n",
       "      <td>62.32</td>\n",
       "      <td>61.57</td>\n",
       "      <td>61.74</td>\n",
       "      <td>64.32</td>\n",
       "      <td>64.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-07-08_00-25-04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.005</td>\n",
       "      <td>768</td>\n",
       "      <td>71.48</td>\n",
       "      <td>71.34</td>\n",
       "      <td>71.43</td>\n",
       "      <td>71.06</td>\n",
       "      <td>71.38</td>\n",
       "      <td>71.16</td>\n",
       "      <td>73.33</td>\n",
       "      <td>73.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-07-08_00-25-05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>27.59</td>\n",
       "      <td>29.28</td>\n",
       "      <td>30.10</td>\n",
       "      <td>31.91</td>\n",
       "      <td>28.12</td>\n",
       "      <td>30.39</td>\n",
       "      <td>32.25</td>\n",
       "      <td>36.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-07-08_00-25-06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.010</td>\n",
       "      <td>768</td>\n",
       "      <td>65.48</td>\n",
       "      <td>66.39</td>\n",
       "      <td>66.03</td>\n",
       "      <td>66.79</td>\n",
       "      <td>65.83</td>\n",
       "      <td>66.72</td>\n",
       "      <td>68.29</td>\n",
       "      <td>68.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-07-08_00-25-07</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>67.34</td>\n",
       "      <td>68.44</td>\n",
       "      <td>67.61</td>\n",
       "      <td>68.47</td>\n",
       "      <td>67.61</td>\n",
       "      <td>68.72</td>\n",
       "      <td>71.10</td>\n",
       "      <td>70.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-07-08_00-25-08</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.001</td>\n",
       "      <td>768</td>\n",
       "      <td>71.69</td>\n",
       "      <td>71.36</td>\n",
       "      <td>71.52</td>\n",
       "      <td>70.87</td>\n",
       "      <td>71.66</td>\n",
       "      <td>71.24</td>\n",
       "      <td>73.58</td>\n",
       "      <td>73.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-07-08_00-25-09</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>9.03</td>\n",
       "      <td>17.02</td>\n",
       "      <td>8.93</td>\n",
       "      <td>17.84</td>\n",
       "      <td>9.12</td>\n",
       "      <td>17.18</td>\n",
       "      <td>17.26</td>\n",
       "      <td>18.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-07-08_00-25-10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.005</td>\n",
       "      <td>768</td>\n",
       "      <td>60.55</td>\n",
       "      <td>59.62</td>\n",
       "      <td>61.38</td>\n",
       "      <td>60.59</td>\n",
       "      <td>60.93</td>\n",
       "      <td>60.03</td>\n",
       "      <td>62.73</td>\n",
       "      <td>64.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-07-08_00-25-11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.43</td>\n",
       "      <td>5.96</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.77</td>\n",
       "      <td>6.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-07-08_00-25-12</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.010</td>\n",
       "      <td>768</td>\n",
       "      <td>25.93</td>\n",
       "      <td>25.45</td>\n",
       "      <td>26.75</td>\n",
       "      <td>26.64</td>\n",
       "      <td>26.36</td>\n",
       "      <td>26.08</td>\n",
       "      <td>27.22</td>\n",
       "      <td>29.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 job_id  learning rate  weight decay batch size  normal  \\\n",
       "0   2025-07-08_00-24-53           0.03         0.001        128   73.52   \n",
       "1   2025-07-08_00-24-55           0.03         0.001        768   69.67   \n",
       "2   2025-07-08_00-24-56           0.03         0.005        128   72.36   \n",
       "3   2025-07-08_00-24-57           0.03         0.005        768   72.28   \n",
       "4   2025-07-08_00-24-58           0.03         0.010        128   67.64   \n",
       "5   2025-07-08_00-24-59           0.03         0.010        768   72.82   \n",
       "6   2025-07-08_00-25-00           0.10         0.001        128   72.95   \n",
       "7   2025-07-08_00-25-01           0.10         0.001        768   71.82   \n",
       "8   2025-07-08_00-25-02           0.10         0.005        128   61.38   \n",
       "9   2025-07-08_00-25-04           0.10         0.005        768   71.48   \n",
       "10  2025-07-08_00-25-05           0.10         0.010        128   27.59   \n",
       "11  2025-07-08_00-25-06           0.10         0.010        768   65.48   \n",
       "12  2025-07-08_00-25-07           0.30         0.001        128   67.34   \n",
       "13  2025-07-08_00-25-08           0.30         0.001        768   71.69   \n",
       "14  2025-07-08_00-25-09           0.30         0.005        128    9.03   \n",
       "15  2025-07-08_00-25-10           0.30         0.005        768   60.55   \n",
       "16  2025-07-08_00-25-11           0.30         0.010        128    1.69   \n",
       "17  2025-07-08_00-25-12           0.30         0.010        768   25.93   \n",
       "\n",
       "    negative  hybrid normal  hybrid negative  synergy normal  \\\n",
       "0      73.39          73.29            73.01           73.36   \n",
       "1      70.15          69.68            69.70           69.56   \n",
       "2      71.67          72.41            71.75           72.48   \n",
       "3      72.11          72.05            72.25           72.07   \n",
       "4      68.41          68.18            68.50           67.72   \n",
       "5      72.18          72.78            71.93           72.80   \n",
       "6      72.69          72.71            72.32           73.00   \n",
       "7      71.68          71.75            71.39           71.86   \n",
       "8      61.22          61.83            62.32           61.57   \n",
       "9      71.34          71.43            71.06           71.38   \n",
       "10     29.28          30.10            31.91           28.12   \n",
       "11     66.39          66.03            66.79           65.83   \n",
       "12     68.44          67.61            68.47           67.61   \n",
       "13     71.36          71.52            70.87           71.66   \n",
       "14     17.02           8.93            17.84            9.12   \n",
       "15     59.62          61.38            60.59           60.93   \n",
       "16      2.88           3.43             5.96            1.99   \n",
       "17     25.45          26.75            26.64           26.36   \n",
       "\n",
       "    synergy negative  synergy all  synergy trained all  \n",
       "0              73.16        75.86                75.50  \n",
       "1              70.03        71.83                71.67  \n",
       "2              71.73        74.53                74.37  \n",
       "3              72.16        74.25                74.39  \n",
       "4              68.47        70.05                70.76  \n",
       "5              72.28        74.56                74.44  \n",
       "6              72.54        75.21                74.49  \n",
       "7              71.43        74.12                73.58  \n",
       "8              61.74        64.32                64.97  \n",
       "9              71.16        73.33                73.48  \n",
       "10             30.39        32.25                36.38  \n",
       "11             66.72        68.29                68.67  \n",
       "12             68.72        71.10                70.49  \n",
       "13             71.24        73.58                73.17  \n",
       "14             17.18        17.26                18.76  \n",
       "15             60.03        62.73                64.00  \n",
       "16              3.46         3.77                 6.85  \n",
       "17             26.08        27.22                29.60  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_test_results_csv(file_path):\n",
    "    \"\"\"\n",
    "    Reads the test results from a CSV file and returns a DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # remove loss column \n",
    "        if 'test_loss' in df.columns:\n",
    "            df = df.drop(columns=['test_loss'])\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return pd.DataFrame()\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"File {file_path} is empty.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Create dataframe to hold all results and define columns\n",
    "columns = [\n",
    "    \"job_id\",\n",
    "    \"learning rate\",\n",
    "    \"weight decay\",\n",
    "    \"batch size\",\n",
    "    \"normal\",\n",
    "    \"negative\",\n",
    "    \"hybrid normal\",\n",
    "    \"hybrid negative\",\n",
    "    \"synergy normal\",\n",
    "    \"synergy negative\",\n",
    "    \"synergy all\",\n",
    "    \"synergy trained all\"\n",
    "]\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Go through each directory in the sweep directory\n",
    "job_dirs = [d for d in os.listdir(sweep_dir) if os.path.isdir(os.path.join(sweep_dir, d))]\n",
    "\n",
    "dataset_name = None\n",
    "\n",
    "for job_dir in job_dirs:\n",
    "    job_path = os.path.join(sweep_dir, job_dir)\n",
    "    # load config file\n",
    "    config_file_path = os.path.join(job_path, \"code\\\\config_train.py\")\n",
    "    module_name = job_dir.replace(\"-\", \"_\")\n",
    "    spec = importlib.util.spec_from_file_location(module_name, config_file_path)\n",
    "    config_module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = config_module\n",
    "    spec.loader.exec_module(config_module)\n",
    "    # read test results\n",
    "    test_results_file = os.path.join(job_path, \"metrics\\\\test_metrics.csv\")\n",
    "    test_results_df = read_test_results_csv(test_results_file)\n",
    "\n",
    "    if dataset_name is None:\n",
    "        dataset_name = config_module.dataset_name\n",
    "\n",
    "    new_row = pd.DataFrame([{\n",
    "        \"job_id\": job_dir,\n",
    "        \"learning rate\": config_module.learning_rate,\n",
    "        \"weight decay\": config_module.decay,\n",
    "        \"batch size\": config_module.batch_size,\n",
    "        \"normal\": test_results_df.loc[test_results_df[\"model_name\"] == \"normal\", \"test_accuracy\"].values[0] if not test_results_df[test_results_df[\"model_name\"] == \"normal\"].empty else None,\n",
    "        \"negative\": test_results_df.loc[test_results_df[\"model_name\"] == \"negative\", \"test_accuracy\"].values[0] if not test_results_df[test_results_df[\"model_name\"] == \"negative\"].empty else None,\n",
    "        \"hybrid normal\": test_results_df.loc[test_results_df[\"model_name\"] == \"hybrid_nor\", \"test_accuracy\"].values[0] if not test_results_df[test_results_df[\"model_name\"] == \"hybrid_nor\"].empty else None,\n",
    "        \"hybrid negative\": test_results_df.loc[test_results_df[\"model_name\"] == \"hybrid_neg\", \"test_accuracy\"].values[0] if not test_results_df[test_results_df[\"model_name\"] == \"hybrid_neg\"].empty else None,\n",
    "        \"synergy normal\": test_results_df.loc[test_results_df[\"model_name\"] == \"synergy_nor\", \"test_accuracy\"].values[0] if not test_results_df[test_results_df[\"model_name\"] == \"synergy_nor\"].empty else None,\n",
    "        \"synergy negative\": test_results_df.loc[test_results_df[\"model_name\"] == \"synergy_neg\", \"test_accuracy\"].values[0] if not test_results_df[test_results_df[\"model_name\"] == \"synergy_neg\"].empty else None,\n",
    "        \"synergy all\": test_results_df.loc[test_results_df[\"model_name\"] == \"synergy_all\", \"test_accuracy\"].values[0] if not test_results_df[test_results_df[\"model_name\"] == \"synergy_all\"].empty else None,\n",
    "        \"synergy trained all\": test_results_df.loc[test_results_df[\"model_name\"] == \"tr_synergy_all\", \"test_accuracy\"].values[0] if not test_results_df[test_results_df[\"model_name\"] == \"tr_synergy_all\"].empty else None\n",
    "    }])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(\"Results DataFrame:\")\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6de63b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ..\\base_training\\sweep_2025-07-08_00-24-53\\test_results.csv\n"
     ]
    }
   ],
   "source": [
    "# export results to csv\n",
    "results_csv_path = os.path.join(sweep_dir, \"test_results.csv\")\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"Results saved to {results_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63b076b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latex table saved to ..\\base_training\\sweep_2025-07-08_00-24-53\\test_results.tex\n"
     ]
    }
   ],
   "source": [
    "# Convert csv to latex table\n",
    "latex_table_path = os.path.join(sweep_dir, \"test_results.tex\")\n",
    "# before exporting remove the job_id column if it exists\n",
    "if 'job_id' in results_df.columns:\n",
    "    results_df = results_df.drop(columns=['job_id'])\n",
    "\n",
    "# Format columns with different precision\n",
    "results_df_copy = results_df.copy()\n",
    "\n",
    "# Step 1: Format columns with different precisions\n",
    "for col in results_df_copy.columns:\n",
    "    if col == \"weight decay\":\n",
    "        results_df_copy[col] = results_df_copy[col].apply(lambda x: f\"{x:.3f}\" if pd.notnull(x) else \"\")\n",
    "    elif col != \"batch size\":\n",
    "        results_df_copy[col] = results_df_copy[col].apply(lambda x: f\"{x:.2f}\" if pd.notnull(x) else \"\")\n",
    "\n",
    "# Step 2: Bold max values (only for formatted float-like columns)\n",
    "for col in results_df_copy.columns:\n",
    "    # Skip batch size or any clearly non-numeric columns\n",
    "    if col in [\"learning rate\", \"weight decay\", \"batch size\"]:\n",
    "        continue\n",
    "    try:\n",
    "        # Convert formatted strings back to float to find max\n",
    "        numeric_col = results_df_copy[col].replace('', np.nan).astype(float)\n",
    "        max_val = numeric_col.max()\n",
    "        results_df_copy[col] = results_df_copy[col].apply(\n",
    "            lambda x: f\"\\\\textbf{{{x}}}\" if x != '' and float(x) == max_val else x\n",
    "        )\n",
    "    except ValueError:\n",
    "        continue  # Skip columns that can't be converted to float\n",
    "\n",
    "# Step 3: Convert DataFrame to LaTeX table\n",
    "with open(latex_table_path, 'w') as f:\n",
    "    f.write(results_df_copy.to_latex(index=False, escape=False))\n",
    "\n",
    "print(f\"Latex table saved to {latex_table_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987e128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
